---
phase: 01-foundation-context-engine
plan: 04
type: execute
wave: 3
depends_on: ["01-01", "01-03"]
files_modified:
  - src/services/llm-client.ts
  - src/hooks/use-llm-stream.ts
  - src/components/settings/SettingsPanel.tsx
  - src/components/settings/SettingsPanel.css
  - src/components/settings/ConnectionStatus.tsx
  - src/components/settings/ModelSelector.tsx
  - src/components/settings/GenerationParams.tsx
  - src/components/settings/GenerationParams.css
  - src/components/settings/PresetSelector.tsx
  - src/App.tsx
autonomous: true

must_haves:
  truths:
    - "User can enter base URL, API key, and model name in a settings form"
    - "Connection test shows green/red status badge and auto-fetches available models"
    - "Test Generate button validates full pipeline with a simple prompt"
    - "Model selector auto-discovers from /v1/models with manual text input fallback"
    - "User can adjust temperature, max tokens, top_p, frequency penalty, presence penalty"
    - "User can select from built-in presets (Creative, Balanced, Precise) and save custom presets"
    - "System prompt is editable in the textarea component as a markdown file"
  artifacts:
    - path: "src/services/llm-client.ts"
      provides: "OpenAI SDK wrapper with connection test and model discovery"
      exports: ["testConnection", "testGenerate", "fetchModels", "createLLMClient"]
    - path: "src/hooks/use-llm-stream.ts"
      provides: "Streaming generation hook with abort support"
      exports: ["useLLMStream"]
    - path: "src/components/settings/SettingsPanel.tsx"
      provides: "Settings tab content with connection form and parameters"
      exports: ["SettingsPanel"]
    - path: "src/components/settings/GenerationParams.tsx"
      provides: "Generation parameter controls with presets"
      exports: ["GenerationParams"]
  key_links:
    - from: "src/services/llm-client.ts"
      to: "openai"
      via: "OpenAI SDK client creation"
      pattern: "new OpenAI"
    - from: "src/hooks/use-llm-stream.ts"
      to: "src/services/llm-client.ts"
      via: "import createLLMClient"
      pattern: "import.*llm-client"
    - from: "src/components/settings/SettingsPanel.tsx"
      to: "src/stores/settings-store.ts"
      via: "Zustand store for connection config"
      pattern: "useSettingsStore"
    - from: "src/components/settings/GenerationParams.tsx"
      to: "src/services/db.ts"
      via: "Load/save presets from SQLite"
      pattern: "getDatabase.*presets"
---

<objective>
Build the LLM integration layer and settings UI: the OpenAI SDK wrapper with connection testing and model discovery, the streaming generation hook with abort support, the full settings panel with connection form and status badge, generation parameter controls with presets, and the system prompt editor.

Purpose: This plan delivers the user's primary interaction with their local LLM — configuring the connection, testing it works, adjusting generation parameters, and having a streaming hook ready for the generation workspace.

Output: Working LLM connection UI with status indicators, streaming generation capability, and parameter/preset management.
</objective>

<execution_context>
@/home/flight/.claude/get-shit-done/workflows/execute-plan.md
@/home/flight/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-foundation-context-engine/01-CONTEXT.md
@.planning/phases/01-foundation-context-engine/01-RESEARCH.md
@.planning/phases/01-foundation-context-engine/01-01-SUMMARY.md
@.planning/phases/01-foundation-context-engine/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM client service and streaming generation hook</name>
  <files>
    src/services/llm-client.ts
    src/hooks/use-llm-stream.ts
  </files>
  <action>
    **LLM client service (src/services/llm-client.ts):**

    Export `createLLMClient(baseURL, apiKey)` — returns an OpenAI client instance with `dangerouslyAllowBrowser: true` (required for browser-side use with local LLMs — acceptable since this connects to localhost).

    Export `testConnection(baseURL, apiKey)` — quick connection test:
    - Attempts `client.models.list()`
    - Returns `{ success: boolean, models?: string[], error?: string }`
    - Timeout after 5 seconds (local LLMs should respond fast)

    Export `testGenerate(baseURL, apiKey, model)` — full pipeline validation:
    - Sends a simple non-streaming completion: "Reply with OK if you can read this."
    - max_tokens: 10
    - Returns `{ success: boolean, output?: string, latencyMs?: number, error?: string }`

    Export `fetchModels(baseURL, apiKey)` — auto-discover available models:
    - Calls `/v1/models` endpoint
    - Returns string[] of model IDs
    - Returns empty array on failure (fallback to manual input)

    **Streaming generation hook (src/hooks/use-llm-stream.ts):**

    Export `useLLMStream()` hook:
    - Reads connection settings from useSettingsStore
    - State: isGenerating, isStopping, error, tokensGenerated, startTime
    - `generate(messages, params, onChunk)`:
      - Creates AbortController
      - Calls client.chat.completions.create with stream: true
      - Buffers chunks (accumulate ~50 chars before calling onChunk for smooth append per user decision)
      - Tracks tokensGenerated count and startTime for tokens/sec calculation
      - On AbortError: set isStopping briefly, then idle
      - On other errors: set error state with message
    - `stop()`: abort the controller, set isStopping state, clear after 500ms
    - Computed: `tokensPerSecond` — tokensGenerated / elapsed seconds
    - Generate button disabled when isGenerating OR isStopping (per research pitfall)

    The hook does NOT persist generations to DB — that's the generation workspace's responsibility (Plan 06). This hook is purely the streaming transport.
  </action>
  <verify>
    `npm run build` — no TypeScript errors.
    Import both modules in a test component — no runtime import errors.
    If a local LLM is running: `testConnection` returns models list; `testGenerate` returns OK response.
  </verify>
  <done>LLM client creates OpenAI instances, testConnection validates endpoint and fetches models, testGenerate validates full pipeline, streaming hook handles chunk buffering and abort with proper state transitions.</done>
</task>

<task type="auto">
  <name>Task 2: Settings panel UI with connection form, model selector, generation parameters, and presets</name>
  <files>
    src/components/settings/SettingsPanel.tsx
    src/components/settings/SettingsPanel.css
    src/components/settings/ConnectionStatus.tsx
    src/components/settings/ModelSelector.tsx
    src/components/settings/GenerationParams.tsx
    src/components/settings/GenerationParams.css
    src/components/settings/PresetSelector.tsx
    src/App.tsx
  </files>
  <action>
    **SettingsPanel (src/components/settings/SettingsPanel.tsx):**
    - Renders as the content for the "Settings" tab in the left pane
    - Sections (vertical layout, scrollable):
      1. **Connection** — base URL input, API key input (password type with show/hide toggle), ConnectionStatus badge, "Test Connection" button, "Test Generate" button
      2. **Model** — ModelSelector component
      3. **Generation Parameters** — GenerationParams component (collapsible section, starts collapsed)
      4. **System Prompt** — A button/link that loads the system prompt into the right-pane textarea for editing. System prompt stored as a version-tracked content item (contentId: 'system-prompt') with a default value: "You are a creative writing assistant. Write engaging, vivid prose that follows the provided story context."
    - All settings read/write via useSettingsStore
    - Form inputs styled consistently: dark inputs with subtle borders, labels above inputs

    **ConnectionStatus (src/components/settings/ConnectionStatus.tsx):**
    - Small status badge: green circle + "Connected" / red circle + "Error" / gray circle + "Not connected"
    - Shows connection error message below badge when in error state
    - Inline component (sits next to the connection inputs)

    **ModelSelector (src/components/settings/ModelSelector.tsx):**
    - If models discovered (from testConnection): render a dropdown/select with available models
    - If no models discovered (endpoint doesn't support /v1/models): render a text input for manual model name entry
    - "Refresh Models" button to re-fetch from endpoint
    - Selected model saved to settings store

    **GenerationParams (src/components/settings/GenerationParams.tsx):**
    - Collapsible section (collapsed by default, per user decision: "accessible but tucked away")
    - Slider + number input for each parameter:
      - Temperature: 0.0 - 2.0, step 0.1
      - Max Tokens: 64 - 8192, step 64
      - Top P: 0.0 - 1.0, step 0.05
      - Frequency Penalty: 0.0 - 2.0, step 0.1
      - Presence Penalty: 0.0 - 2.0, step 0.1
    - Each slider shows current value
    - Parameters stored in a Zustand store (can add to settings-store or create params-store)

    **PresetSelector (src/components/settings/PresetSelector.tsx):**
    - Dropdown showing built-in presets (Creative, Balanced, Precise) + any user-created presets
    - Selecting a preset fills all parameter sliders with preset values
    - "Save as Preset" button: prompts for name, saves current parameter values to SQLite presets table
    - "Delete" button next to user presets (built-in presets cannot be deleted)

    **Update App.tsx:**
    - Wire SettingsPanel as the content for the "settings" tab in TabSystem
    - Wire the system prompt textarea: when user clicks "Edit System Prompt" in settings, the right pane loads the system prompt content (contentId: 'system-prompt')
  </action>
  <verify>
    `npm run build` — no errors.
    Settings tab shows connection form with all fields.
    Connection status badge updates after clicking "Test Connection".
    Model selector shows dropdown when models discovered, text input when not.
    Generation parameters section is collapsible and all sliders work.
    Preset selector loads built-in presets and applies values to sliders.
    System prompt edit button loads content in the right-pane textarea.
    All settings persist across page refresh (Zustand persist).
  </verify>
  <done>Settings panel renders with connection form, status badge, model selector, collapsible generation parameters with working sliders, preset selector (3 built-in + user presets), and system prompt editing via the textarea. All settings persist.</done>
</task>

</tasks>

<verification>
1. `npm run build` — zero errors
2. LLM client testConnection returns success/failure with model list
3. Streaming hook generates text with chunk buffering and abort support
4. Settings panel: all form fields functional and persisted
5. Connection status badge shows green/red/gray correctly
6. Model selector auto-discovers or falls back to manual input
7. Generation parameter sliders adjust values, presets apply correctly
8. System prompt editable in textarea
</verification>

<success_criteria>
- LLM client wraps OpenAI SDK with testConnection, testGenerate, fetchModels
- Streaming hook handles chunk buffering (~50 chars), abort, and state transitions
- Settings panel: connection form with base URL, API key, model name
- Two-level connection test: quick badge + test generate button
- Model auto-discovery from /v1/models with manual fallback
- Generation parameters: 5 sliders in collapsible section
- 3 built-in presets + user-created presets
- System prompt editable as markdown in the textarea component
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-context-engine/01-04-SUMMARY.md`
</output>
