---
phase: 01-foundation-context-engine
plan: 06
type: execute
wave: 4
depends_on: ["01-04", "01-05"]
files_modified:
  - src/components/generation/GenerationWorkspace.tsx
  - src/components/generation/GenerationWorkspace.css
  - src/components/generation/PromptHistory.tsx
  - src/components/generation/GenerationControls.tsx
  - src/components/generation/GenerationStats.tsx
  - src/hooks/use-generation.ts
  - src/hooks/use-prompt-history.ts
  - src/stores/generation-store.ts
  - src/App.tsx
autonomous: false

must_haves:
  truths:
    - "User can type a prompt and generate text with streaming output in the textarea"
    - "Streaming text renders chunk-by-chunk (smooth append) in the CodeMirror textarea"
    - "Stop button halts generation mid-stream"
    - "Live token count and tokens/second readout visible during generation"
    - "Prompt history shows previous prompts in left pane — click to reuse"
    - "After generation: user can copy output, regenerate, or edit text inline"
    - "Inline edits tracked as versions — original generation preserved"
    - "All generations (prompt + output) persisted to database across sessions"
    - "Context engine packs system prompt + recent text + history into LLM call with budget visualization"
    - "Context visualization updates live with actual packed context data"
    - "Connection failure mid-use shows inline error with retry button"
  artifacts:
    - path: "src/components/generation/GenerationWorkspace.tsx"
      provides: "Left pane content for generation tab"
      exports: ["GenerationWorkspace"]
    - path: "src/components/generation/PromptHistory.tsx"
      provides: "List of previous prompts with click-to-reuse"
      exports: ["PromptHistory"]
    - path: "src/components/generation/GenerationControls.tsx"
      provides: "Generate/Stop/Regenerate/Copy buttons and stats"
      exports: ["GenerationControls"]
    - path: "src/hooks/use-generation.ts"
      provides: "Orchestration hook tying LLM stream, context engine, and persistence together"
      exports: ["useGeneration"]
    - path: "src/hooks/use-prompt-history.ts"
      provides: "CRUD for prompt/generation history from SQLite"
      exports: ["usePromptHistory"]
    - path: "src/stores/generation-store.ts"
      provides: "Generation workspace state"
      exports: ["useGenerationStore"]
  key_links:
    - from: "src/hooks/use-generation.ts"
      to: "src/hooks/use-llm-stream.ts"
      via: "Delegates streaming to LLM stream hook"
      pattern: "useLLMStream"
    - from: "src/hooks/use-generation.ts"
      to: "src/services/context-engine.ts"
      via: "Packs context before each generation call"
      pattern: "packContext"
    - from: "src/hooks/use-generation.ts"
      to: "src/services/db.ts"
      via: "Persists generation to SQLite"
      pattern: "getDatabase.*generations"
    - from: "src/components/generation/GenerationWorkspace.tsx"
      to: "src/components/textarea/EnhancedTextarea.tsx"
      via: "Textarea displays generation output"
      pattern: "EnhancedTextarea"
    - from: "src/hooks/use-generation.ts"
      to: "src/components/context-viz/ContextVisualization.tsx"
      via: "Passes packed ContextTier[] to visualization"
      pattern: "ContextTier"
---

<objective>
Build the generation workspace — the complete user workflow for generating text. This is where everything comes together: the user types a prompt, the context engine packs the LLM call, streaming output fills the textarea, stats show live performance, and the context visualization displays what the model sees. This plan also adds prompt history in the left pane and all post-generation actions (copy, regenerate, edit).

Purpose: This is the core deliverable of Phase 1 — the user can actually use Storyteller to generate text with full context transparency. All prior plans built infrastructure for this moment.

Output: A fully functional generation workflow with streaming output, context packing, live visualization, and persistent history.
</objective>

<execution_context>
@/home/flight/.claude/get-shit-done/workflows/execute-plan.md
@/home/flight/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-foundation-context-engine/01-CONTEXT.md
@.planning/phases/01-foundation-context-engine/01-RESEARCH.md
@.planning/phases/01-foundation-context-engine/01-04-SUMMARY.md
@.planning/phases/01-foundation-context-engine/01-05-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generation orchestration hook and prompt history</name>
  <files>
    src/hooks/use-generation.ts
    src/hooks/use-prompt-history.ts
    src/stores/generation-store.ts
  </files>
  <action>
    **Generation store (src/stores/generation-store.ts):**
    - Zustand store for generation workspace state
    - State: currentPrompt (string), currentOutput (string), contextTiers (ContextTier[]), maxContextTokens (number, default 4096), isEditing (boolean — user is editing output inline), lastError (string | null)
    - Actions: setCurrentPrompt, appendToOutput, setOutput, setContextTiers, setMaxContextTokens, setIsEditing, setError, clearError, reset

    **usePromptHistory hook (src/hooks/use-prompt-history.ts):**
    - Uses TanStack Query to load generations from SQLite `generations` table
    - queryKey: ['generations']
    - Returns generations ordered by created_at DESC
    - saveGeneration mutation: INSERT into generations with prompt, output, model, parameters (JSON string), token_count
    - Invalidates query on save
    - Pagination: load last 50 generations

    **useGeneration hook (src/hooks/use-generation.ts):**
    - Orchestration hook that ties everything together
    - Uses: useLLMStream, useGenerationStore, usePromptHistory, useSettingsStore, useCompressionLog
    - `generate(prompt)` flow:
      1. Read system prompt from version history (contentId: 'system-prompt', latest version)
      2. Read current output as "recent text" (if continuing a session)
      3. Call `packContext()` from context-engine with tiers:
         - System prompt (priority 100)
         - Recent text verbatim (priority 90, last ~500 tokens of current output)
         - Compressed history (priority 50, summary of earlier output — for Phase 1, use simple truncation; LLM-based summarization in Phase 3)
      4. Update contextTiers in generation store (feeds visualization)
      5. Build messages array from packed context: system message (system prompt tier) + user message (prompt with recent text context)
      6. Call useLLMStream.generate with messages and current generation params
      7. onChunk: append to generation store's currentOutput
      8. On completion: save to prompt history (SQLite), create a version for the output (version_type='generation')
      9. Log any compression events via useCompressionLog.addEvent
    - `regenerate()`: re-run generate with the same prompt (new output)
    - `stop()`: delegates to useLLMStream.stop
    - `copyOutput()`: navigator.clipboard.writeText(currentOutput)
    - Error handling: on LLM error, set lastError in store, show inline where output was streaming
    - Retry: clearError + re-run generate
    - Connection failure mid-generation: set error with "Connection lost" message + retry button context

    **Context packing strategy (Phase 1):**
    - System prompt: always included at priority 100
    - Recent text: last 500 tokens of current output at priority 90 (verbatim)
    - Compressed history: if output > 500 tokens, everything before the recent text gets truncated to fit budget at priority 50 (simple truncation for now — LLM summarization replaces this in Phase 3)
    - Max context tokens: configurable, default 4096 (user can adjust for their model)
    - Reserve tokens for generation: maxContextTokens - max_tokens parameter = context budget
  </action>
  <verify>
    `npm run build` — no errors.
    Import useGeneration in a test component — hook initializes without errors.
    Call generate with a test prompt — context tiers computed, LLM stream initiated, output accumulated.
  </verify>
  <done>Generation orchestration hook coordinates context packing, streaming generation, persistence, and error handling. Prompt history loads/saves from SQLite. Generation store manages workspace state.</done>
</task>

<task type="auto">
  <name>Task 2: Generation workspace UI with controls, stats, and prompt history</name>
  <files>
    src/components/generation/GenerationWorkspace.tsx
    src/components/generation/GenerationWorkspace.css
    src/components/generation/PromptHistory.tsx
    src/components/generation/GenerationControls.tsx
    src/components/generation/GenerationStats.tsx
    src/App.tsx
  </files>
  <action>
    **GenerationWorkspace (src/components/generation/GenerationWorkspace.tsx):**
    - Renders as the content for the "Generation" tab in the left pane
    - Layout (top to bottom):
      1. Prompt input area — an EnhancedTextarea instance (contentId: 'current-prompt', smaller height ~120px) for typing prompts
      2. GenerationControls (Generate/Stop/Regen/Copy buttons + stats)
      3. PromptHistory list (scrollable, takes remaining space)
    - Uses useGeneration hook for all actions
    - When user clicks a prompt from history: load it into the prompt textarea, load the associated output into the right-pane textarea

    **GenerationControls (src/components/generation/GenerationControls.tsx):**
    - Horizontal button bar:
      - "Generate" button (primary, disabled during generation/stopping)
      - "Stop" button (visible only during generation, replaces Generate)
      - "Regenerate" button (visible after generation, re-runs same prompt)
      - "Copy" button (visible after generation, copies output to clipboard)
    - Below buttons: GenerationStats component
    - Error display: if lastError set, show inline error message with "Retry" button
    - All buttons have keyboard shortcuts (Enter to generate, Escape to stop)

    **GenerationStats (src/components/generation/GenerationStats.tsx):**
    - Compact stats bar visible during and after generation:
      - Token count: "X tokens generated"
      - Speed: "Y tokens/sec" (live during generation, final after)
      - Elapsed time: "Zs" (live counter during generation)
    - Stats reset when new generation starts
    - When not generating and no output: hidden

    **PromptHistory (src/components/generation/PromptHistory.tsx):**
    - Scrollable list of previous generations
    - Each item shows:
      - First line of prompt (truncated to ~60 chars)
      - Model name
      - Token count
      - Relative timestamp ("2m ago", "1h ago")
    - Click to load: sets prompt textarea and output textarea to the historical values
    - When clicked, the right-pane textarea switches to showing that generation's output (with version history intact)
    - Empty state: "No generations yet. Type a prompt and click Generate."

    **Update App.tsx — Full wiring:**
    - Wire GenerationWorkspace as the content for the "generation" tab
    - Right pane now has two states:
      1. When in generation tab: top section is EnhancedTextarea showing generation output (contentId: 'generation-output'), bottom section is ContextVisualization
      2. When in settings tab and editing system prompt: top section is EnhancedTextarea showing system prompt (contentId: 'system-prompt')
    - Pass contextTiers from generation store to ContextVisualization (replace mock data with live data)
    - Pass maxContextTokens from generation store to ContextVisualization
    - Pass compressionEvents from useCompressionLog to ContextVisualization

    **Streaming UX:**
    - During generation: textarea shows output accumulating chunk-by-chunk
    - Textarea is read-only during generation (prevent edits while streaming)
    - After generation: textarea becomes editable
    - Inline edits after generation create new versions (version_type='auto'), preserving the original generation output
  </action>
  <verify>
    `npm run build` — no errors.
    Generation workspace renders in the left pane with prompt input, controls, and history.
    Type a prompt and click Generate:
    - If LLM connected: streaming output appears in textarea, stats show live token count and speed
    - If LLM not connected: error message with retry button
    Stop button halts generation.
    Regenerate re-runs the same prompt.
    Copy button copies output to clipboard.
    Prompt history populates after generation.
    Clicking history item loads prompt and output.
    Context visualization shows live tier data during/after generation.
  </verify>
  <done>Generation workspace renders prompt input, controls, stats, and history in left pane. Streaming output fills textarea in right pane. Context visualization shows live packed context data. All generations persisted to SQLite. Post-generation actions (copy, regenerate, edit) functional.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: End-to-end Phase 1 verification</name>
  <files>src/App.tsx</files>
  <action>
    Human verification of complete Phase 1 generation workflow:
    1. Split-screen app with tabbed navigation (Generation + Settings)
    2. CodeMirror textarea with markdown highlighting, version navigation, auto-save
    3. LLM connection settings with status badge, model discovery, and test buttons
    4. Generation parameters with sliders and presets (Creative/Balanced/Precise)
    5. System prompt editing in the textarea
    6. Streaming text generation with stop button, live stats
    7. Context engine packing with priority-based tier allocation
    8. Context visualization (stacked bar, itemized list, text inspection, compression log)
    9. Prompt history with click-to-reuse
    10. All data persisted in SQLite WASM (OPFS)
  </action>
  <verify>
    1. Open the app at http://localhost:5173
    2. Go to Settings tab — enter your local LLM base URL (e.g., http://localhost:1234/v1)
    3. Click "Test Connection" — verify green status badge and model list appears
    4. Select a model from the dropdown
    5. Click "Test Generate" — verify it shows a success response
    6. Go to Generation tab
    7. Type a creative writing prompt (e.g., "Write the opening paragraph of a mystery novel set in a foggy coastal town")
    8. Click Generate — verify streaming text appears chunk-by-chunk, live token count and tokens/sec update, context visualization bar shows colored segments
    9. Click Stop mid-generation — verify it halts
    10. After generation completes: click "Copy" (paste elsewhere to verify clipboard), click "Regenerate" (new output from same prompt), edit the text inline (verify version nav shows multiple versions), press Ctrl+S (verify explicit save point created)
    11. Check context visualization: expand "Show Details" (verify itemized list with token counts), click "Inspect" on a category (verify full text shown)
    12. Check prompt history — verify your prompts appear with timestamps
    13. Click a history item — verify prompt and output reload
    14. Refresh the page — verify all data persists (prompt history, settings, etc.)
    15. Adjust generation parameters in Settings: select "Creative" preset (verify sliders change), manually adjust temperature, generate again (observe different output)
  </verify>
  <done>Human confirms end-to-end Phase 1 workflow: LLM connection, streaming generation, context visualization, prompt history, version tracking, and data persistence all working correctly. Type "approved" to complete Phase 1, or describe any issues to address.</done>
</task>

</tasks>

<verification>
1. `npm run build` — zero errors, `npm test` — all tests pass
2. Full generation workflow: prompt → context packing → streaming output → persistence
3. Context visualization shows live data from actual generation calls
4. Prompt history populated and clickable
5. Post-generation actions all functional (copy, regenerate, edit, version tracking)
6. Error handling works (inline error + retry on connection failure)
7. All data persists across page refresh (SQLite WASM + OPFS)
8. Settings persist across refresh (Zustand + localStorage)
9. Human verification confirms end-to-end workflow
</verification>

<success_criteria>
- User types prompt, clicks Generate, sees streaming output in textarea
- Stop button halts generation, stats show final count
- Context engine packs tiers by priority, visualization shows actual data
- Prompt history lists all generations with click-to-reuse
- Copy and Regenerate buttons work
- Inline edits create versions, Ctrl+S creates explicit save
- Connection errors show inline with retry
- All data persists in SQLite across sessions
- Human verifies full end-to-end flow
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-context-engine/01-06-SUMMARY.md`
</output>
