---
phase: 01-foundation-context-engine
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/services/tokens.ts
  - src/services/context-engine.ts
  - src/services/__tests__/tokens.test.ts
  - src/services/__tests__/context-engine.test.ts
autonomous: true

must_haves:
  truths:
    - "Token counting returns accurate counts for text input using BPE encoding"
    - "Context packing fits content into a token budget respecting tier priorities"
    - "Hierarchical compression reduces lower-priority tiers when budget is exceeded"
    - "Compression preserves high-priority tiers (system prompt, recent text) at full fidelity"
  artifacts:
    - path: "src/services/tokens.ts"
      provides: "Token counting and truncation utilities"
      exports: ["countTokens", "truncateToTokens"]
    - path: "src/services/context-engine.ts"
      provides: "Context packing and hierarchical compression"
      exports: ["packContext", "compressToFit", "createContextTier"]
    - path: "src/services/__tests__/tokens.test.ts"
      provides: "Token counting tests"
    - path: "src/services/__tests__/context-engine.test.ts"
      provides: "Context engine tests"
  key_links:
    - from: "src/services/context-engine.ts"
      to: "src/services/tokens.ts"
      via: "import countTokens, truncateToTokens"
      pattern: "import.*from.*tokens"
---

<objective>
Build the context engine — the core business logic for token counting, context packing, and hierarchical compression. This is the most critical technical component: it determines what content enters each LLM call and how historical content is compressed to fit small context windows (4K-8K tokens).

Purpose: The context engine is the "secret sauce" of Storyteller. Without intelligent context packing, small local LLMs lose track of the story. This plan uses TDD to ensure correctness of the packing algorithm, priority-based tier allocation, and compression behavior.

Output: Tested, working token counting and context packing services with no UI dependencies.
</objective>

<execution_context>
@/home/flight/.claude/get-shit-done/workflows/execute-plan.md
@/home/flight/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-foundation-context-engine/01-CONTEXT.md
@.planning/phases/01-foundation-context-engine/01-RESEARCH.md
</context>

<feature>
  <name>Context Engine: Token Counting + Context Packing + Hierarchical Compression</name>
  <files>
    src/services/tokens.ts
    src/services/context-engine.ts
    src/services/__tests__/tokens.test.ts
    src/services/__tests__/context-engine.test.ts
  </files>
  <behavior>
    **Token counting (src/services/tokens.ts):**
    - countTokens(text, model?) -> number: Returns BPE token count using js-tiktoken
    - truncateToTokens(text, maxTokens, model?) -> string: Truncates text to fit within token limit
    - Default model encoding: o200k_base (GPT-4o family) with 15% safety buffer for non-OpenAI models
    - Must free encoding after use to prevent WASM memory leaks
    - Edge cases: empty string -> 0, very long text (10K+ tokens) -> correct count

    Cases:
    - countTokens("hello world") -> ~2 tokens
    - countTokens("") -> 0
    - truncateToTokens("a b c d e f g h", 3) -> truncated to ~3 tokens
    - truncateToTokens("short", 1000) -> "short" (unchanged, under limit)

    **Context packing (src/services/context-engine.ts):**
    - createContextTier(label, content, priority, color) -> ContextTier: Creates a tier with auto-calculated token count
    - packContext(tiers, maxTokens) -> { packed: ContextTier[], totalTokens: number, overflow: boolean }:
      - Sort tiers by priority (highest first)
      - If total fits within maxTokens, return all tiers unchanged
      - If over budget, trim from lowest-priority tiers first
      - Use truncateToTokens for trimming (not character-based truncation)
      - Never trim a tier below 0 tokens
      - Return overflow=true if even after trimming all trimmable tiers, total exceeds budget
    - compressToFit(tiers, maxTokens) -> ContextTier[]: Same as packContext but removes tiers entirely (starting from lowest priority) if truncation isn't enough

    Cases:
    - packContext([system(100), recent(200), history(300)], 600) -> all tiers unchanged, overflow=false
    - packContext([system(100), recent(200), history(300)], 400) -> system + recent unchanged, history trimmed to 100, overflow=false
    - packContext([system(100), recent(200), history(300)], 150) -> system unchanged, recent trimmed, history removed, overflow=false
    - packContext([system(500)], 100) -> system trimmed to 100, overflow=true (couldn't fit even highest priority)

    **Priority levels (standard):**
    - 100: System prompt (never compressed first)
    - 90: Recent text verbatim
    - 70: Story outline (Phase 2+)
    - 50: Compressed history (medium-term)
    - 30: Long-term compressed summary

    Install vitest as dev dependency for testing. Configure in vite.config.ts.
  </behavior>
  <implementation>
    **tokens.ts:**
    - Use `encoding_for_model` from js-tiktoken with try/catch fallback to o200k_base for unknown models
    - Cache encoding instance per model (create once, reuse) to avoid repeated WASM allocation — but provide a `freeEncodings()` cleanup function
    - Actually, for simplicity and memory safety: create encoding per call, free after. Caching optimization can come later.

    **context-engine.ts:**
    - Pure functions, no side effects, no database dependency
    - Import countTokens/truncateToTokens from tokens.ts
    - ContextTier interface imported from src/types/index.ts
    - packContext is the main entry point — used by generation workspace before each LLM call
    - compressToFit is the aggressive version — drops entire tiers when truncation isn't enough

    **Test setup:**
    - Install vitest: `npm install --save-dev vitest @testing-library/react @testing-library/jest-dom jsdom`
    - Add vitest config to vite.config.ts: `test: { globals: true, environment: 'jsdom' }`
    - Add test script to package.json: `"test": "vitest run", "test:watch": "vitest"`
  </implementation>
</feature>

<verification>
1. `npm test` — all tests pass
2. Token counting handles empty strings, normal text, and very long text
3. Context packing respects priority ordering and token budget
4. Tier trimming works correctly (lowest priority trimmed first)
5. No WASM memory leaks (encoding.free() called)
</verification>

<success_criteria>
- All tests pass with `npm test`
- countTokens returns correct BPE token counts
- packContext fits content within budget, preserving high-priority tiers
- compressToFit removes entire tiers when truncation insufficient
- No external dependencies beyond js-tiktoken
- Pure functions with no side effects
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-context-engine/01-02-SUMMARY.md`
</output>
